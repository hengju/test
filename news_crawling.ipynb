{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re \n",
    "import pandas as pd \n",
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('your directory')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "뉴스기사 제목, 날짜, 링크 가져오기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finding_html(soup): \n",
    "\n",
    "    titles = soup.findAll('h2', {'class' : 'categoryArticle__title'})\n",
    "    title_text = [title.get_text() for title in titles] \n",
    "  \n",
    "\n",
    "    dates = soup.findAll('p', {'class' : 'categoryArticle__meta'})\n",
    "    date_text = [date.get_text() for date in dates]\n",
    "\n",
    "\n",
    "    link_text = []       \n",
    "    for x in soup.findAll('div', {'class' : 'categoryArticle'}):\n",
    "        links = x.find('a')['href']\n",
    "        link_text.append(links)\n",
    "\n",
    "    add = {'date' : date_text, 'title' : title_text, 'link' : link_text}\n",
    "\n",
    "    result = pd.DataFrame(add)\n",
    "    \n",
    "    return result \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for page_number in range(1,251): \n",
    "    url = 'https://oilprice.com/Energy/Crude-Oil/Page-{}.html'.format(page_number)\n",
    "    html = requests.get(url).text\n",
    "    soup = BeautifulSoup(html)\n",
    "    \n",
    "    df = finding_html(soup) \n",
    "    \n",
    "    print('다운 받고 있습니다*********')\n",
    "    df.to_csv('link/2020_' + str(page_number) + '.csv', sep=',', encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "뉴스 기사본문 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_content(links): \n",
    "    \n",
    "    content_list = []\n",
    "    for i in range(0,len(links)): \n",
    "        url = links[i]\n",
    "        html = requests.get(url).text\n",
    "        soup = BeautifulSoup(html)\n",
    "        contents = soup.find('div', {'id' : 'article-content'}).get_text()\n",
    "        content_list.append(contents)\n",
    "     \n",
    "    return content_list   \n",
    "\n",
    "\n",
    "for file in os.listdir():\n",
    "    for file_num in range(1,251): \n",
    "        raw_data = pd.read_csv('link/2020_{}.csv'.format(file_num))        \n",
    "        links = raw_data['link']\n",
    "        news_content_list = find_content(links)\n",
    "        result = pd.DataFrame({'content' : news_content_list})          \n",
    "        print('다운 받고 있습니다*********')\n",
    "        result.to_csv('content/2020_' + str(file_num) + '.csv', sep=',', encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CSV파일로 저장하기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,251): \n",
    "    # data with date, link, url \n",
    "    left_data = pd.read_csv('link/2020_{}.csv'.format(i), names=['index', 'date', 'title', 'link'], dtype=str, header=0)\n",
    "    left_data = left_data.set_index('index')\n",
    "\n",
    "    # data with content \n",
    "    right_data = pd.read_csv('content/2020_{}.csv'.format(i), names=['index', 'content'], dtype=str, header=0)\n",
    "    right_data = right_data.set_index('index')\n",
    "\n",
    "    aggregated_df = pd.merge(left_data, right_data, how='outer', on='index')\n",
    "    \n",
    "    print('파일 생성 중*********')\n",
    "    aggregated_df.to_csv('link_and_content/2020_{}.csv'.format(i), sep=',', encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CSV 파일 합치기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  \n",
    "import glob  \n",
    "import os \n",
    "import re\n",
    "\n",
    "all_data = pd.DataFrame()  \n",
    "for f in glob.glob('link_and_content/2020_*.csv'): # 예를들어 202001, 202002, 202003 로 된 파일이면 2019_*  \n",
    "    df = pd.read_csv(f, dtype=str)  \n",
    "    all_data = all_data.append(df, ignore_index=False) \n",
    "    all_data.to_csv('link_and_content/aggregated.csv', mode= 'w', encoding='utf-8-sig')\n",
    "    \n",
    "    \n",
    "    \n",
    "\"\"\"\n",
    "# 기존 파일 삭제\n",
    "os.chdir('./link_and_content')\n",
    "for filename in os.listdir():\n",
    "    result = re.search('[0-9]+.csv', filename)\n",
    "    if result:\n",
    "        os.remove(filename) \n",
    "\"\"\"    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
